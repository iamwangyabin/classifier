{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f8de826-613b-4792-b9ea-2ee3d3f60a20",
   "metadata": {},
   "source": [
    "# 1. Ojha\n",
    "\n",
    "数据集其实real都是同一组，我是事先将其重组好\n",
    "\n",
    "Real 同一组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "940be098-e087-4966-91f2-92eee743b1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "root='./'\n",
    "Ojha_dict = {}\n",
    "for subtask in os.listdir(os.path.join(root, 'Ojha')):\n",
    "    temp_dict={}\n",
    "    for img_path in os.listdir(os.path.join(root, 'Ojha', subtask, '0_real')):\n",
    "        if os.path.isfile(os.path.join(root, 'Ojha', subtask, '0_real', img_path)):\n",
    "            temp_dict[os.path.join(subtask, '0_real', img_path).replace(os.sep, '/')] = 0\n",
    "    for img_path in os.listdir(os.path.join(root, 'Ojha', subtask, '1_fake')):\n",
    "        if os.path.isfile(os.path.join(root, 'Ojha', subtask, '1_fake', img_path)):\n",
    "            temp_dict[os.path.join(subtask, '1_fake', img_path).replace(os.sep, '/')] = 1\n",
    "    Ojha_dict[subtask] = temp_dict\n",
    "\n",
    "\n",
    "with open(os.path.join(root, 'Ojha', 'test.json'), 'w') as json_file:\n",
    "    json.dump(Ojha_dict, json_file, indent=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c7b6a6-1a2f-4904-92df-5d81a566e07e",
   "metadata": {},
   "source": [
    "# 2. UADFV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddcd9e76-26b0-47d2-91ac-7228c431be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "UADFV_dict = {}\n",
    "temp_dict={}\n",
    "for sub_folder in os.listdir(os.path.join(root, 'UADFV', 'fake', 'frames')):\n",
    "    for img_path in os.listdir(os.path.join(root, 'UADFV', 'fake', 'frames', sub_folder)):\n",
    "        if os.path.isfile(os.path.join(root, 'UADFV', 'fake', 'frames', sub_folder, img_path)):\n",
    "            temp_dict[os.path.join('fake', 'frames', sub_folder, img_path).replace(os.sep, '/')] = 1\n",
    "\n",
    "for sub_folder in os.listdir(os.path.join(root, 'UADFV', 'real', 'frames')):\n",
    "    for img_path in os.listdir(os.path.join(root, 'UADFV', 'real', 'frames', sub_folder)):\n",
    "        if os.path.isfile(os.path.join(root, 'UADFV', 'real', 'frames', sub_folder, img_path)):\n",
    "            temp_dict[os.path.join('real', 'frames', sub_folder, img_path).replace(os.sep, '/')] = 0\n",
    "\n",
    "UADFV_dict['all'] = temp_dict\n",
    "\n",
    "\n",
    "with open(os.path.join(root, 'UADFV', 'test.json'), 'w') as json_file:\n",
    "    json.dump(UADFV_dict, json_file, indent=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5024c488-5c4e-48b6-8873-f0e66c57da8a",
   "metadata": {},
   "source": [
    "# 3. DiffusionForensics  \n",
    "\n",
    "Real 同一组\n",
    "这个数据集比较复杂，因为有三大类real，然后每个real都有很多fake\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eb097b9c-5661-4815-80b1-41f32e70bc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "root='./'\n",
    "\n",
    "DiffusionForensics_dict = {}\n",
    "temp_dict={}\n",
    "\n",
    "for ii in os.listdir(os.path.join(root, 'DiffusionForensics', 'imagenet', 'real')):\n",
    "    for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'imagenet', 'real', ii)):\n",
    "        temp_dict[os.path.join('imagenet', 'real', ii, img_path).replace(os.sep, '/')] = 0\n",
    "\n",
    "\n",
    "for ii in os.listdir(os.path.join(root, 'DiffusionForensics', 'imagenet', 'sdv1')):\n",
    "    for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'imagenet', 'sdv1', ii)):\n",
    "        temp_dict[os.path.join('imagenet', 'sdv1', ii, img_path).replace(os.sep, '/')] = 1\n",
    "\n",
    "\n",
    "DiffusionForensics_dict['imagenet_sdv1'] = temp_dict\n",
    "\n",
    "temp_dict={}\n",
    "\n",
    "for ii in os.listdir(os.path.join(root, 'DiffusionForensics', 'imagenet', 'real')):\n",
    "    for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'imagenet', 'real', ii)):\n",
    "        temp_dict[os.path.join('imagenet', 'real', ii, img_path).replace(os.sep, '/')] = 0\n",
    "\n",
    "\n",
    "for ii in os.listdir(os.path.join(root, 'DiffusionForensics', 'imagenet', 'sdv1')):\n",
    "    for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'imagenet', 'adm', ii)):\n",
    "        temp_dict[os.path.join('imagenet', 'adm', ii, img_path).replace(os.sep, '/')] = 1\n",
    "\n",
    "\n",
    "DiffusionForensics_dict['imagenet_adm'] = temp_dict\n",
    "\n",
    " ############################################################################################\n",
    "\n",
    "\n",
    "temp_dict={}\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'lsun_bedroom', 'real')):\n",
    "    temp_dict[os.path.join('lsun_bedroom', 'real', img_path).replace(os.sep, '/')] = 0\n",
    "\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'lsun_bedroom', 'sdv1_new1')):\n",
    "    temp_dict[os.path.join('lsun_bedroom', 'sdv1_new1', img_path).replace(os.sep, '/')] = 1\n",
    "\n",
    "\n",
    "DiffusionForensics_dict['lsun_bedroom_sdv1_new1'] = temp_dict\n",
    "\n",
    "temp_dict={}\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'lsun_bedroom', 'real')):\n",
    "    temp_dict[os.path.join('lsun_bedroom', 'real', img_path).replace(os.sep, '/')] = 0\n",
    "\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'lsun_bedroom', 'sdv1_new2')):\n",
    "    temp_dict[os.path.join('lsun_bedroom', 'sdv1_new2', img_path).replace(os.sep, '/')] = 1\n",
    "\n",
    "\n",
    "DiffusionForensics_dict['lsun_bedroom_sdv1_new2'] = temp_dict\n",
    "\n",
    "temp_dict={}\n",
    "\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'lsun_bedroom', 'real')):\n",
    "    temp_dict[os.path.join('lsun_bedroom', 'real', img_path).replace(os.sep, '/')] = 0\n",
    "\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'lsun_bedroom', 'adm')):\n",
    "    temp_dict[os.path.join('lsun_bedroom', 'adm', img_path).replace(os.sep, '/')] = 1\n",
    "\n",
    "\n",
    "DiffusionForensics_dict['lsun_bedroom_adm'] = temp_dict\n",
    "\n",
    "temp_dict={}\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'lsun_bedroom', 'real')):\n",
    "    temp_dict[os.path.join('lsun_bedroom', 'real', img_path).replace(os.sep, '/')] = 0\n",
    "\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'lsun_bedroom', 'iddpm')):\n",
    "    temp_dict[os.path.join('lsun_bedroom', 'iddpm', img_path).replace(os.sep, '/')] = 1\n",
    "\n",
    "\n",
    "DiffusionForensics_dict['lsun_bedroom_iddpm'] = temp_dict\n",
    "\n",
    "temp_dict={}\n",
    "\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'lsun_bedroom', 'real')):\n",
    "    temp_dict[os.path.join('lsun_bedroom', 'real', img_path).replace(os.sep, '/')] = 0\n",
    "\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'lsun_bedroom', 'ddpm')):\n",
    "    temp_dict[os.path.join('lsun_bedroom', 'ddpm', img_path).replace(os.sep, '/')] = 1\n",
    "\n",
    "\n",
    "DiffusionForensics_dict['lsun_bedroom_ddpm'] = temp_dict\n",
    "\n",
    "temp_dict={}\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'lsun_bedroom', 'real')):\n",
    "    temp_dict[os.path.join('lsun_bedroom', 'real', img_path).replace(os.sep, '/')] = 0\n",
    "\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'lsun_bedroom', 'pndm')):\n",
    "    temp_dict[os.path.join('lsun_bedroom', 'pndm', img_path).replace(os.sep, '/')] = 1\n",
    "\n",
    "\n",
    "DiffusionForensics_dict['lsun_bedroom_pndm'] = temp_dict\n",
    "    \n",
    "temp_dict={}\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'lsun_bedroom', 'real')):\n",
    "    temp_dict[os.path.join('lsun_bedroom', 'real', img_path).replace(os.sep, '/')] = 0\n",
    "\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'lsun_bedroom', 'sdv2')):\n",
    "    temp_dict[os.path.join('lsun_bedroom', 'sdv2', img_path).replace(os.sep, '/')] = 1\n",
    "\n",
    "\n",
    "DiffusionForensics_dict['lsun_bedroom_sdv2'] = temp_dict\n",
    "\n",
    "temp_dict={}\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'lsun_bedroom', 'real')):\n",
    "    temp_dict[os.path.join('lsun_bedroom', 'real', img_path).replace(os.sep, '/')] = 0\n",
    "\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'lsun_bedroom', 'ldm')):\n",
    "    temp_dict[os.path.join('lsun_bedroom', 'ldm', img_path).replace(os.sep, '/')] = 1\n",
    "\n",
    "\n",
    "DiffusionForensics_dict['lsun_bedroom_ldm'] = temp_dict\n",
    "\n",
    "temp_dict={}\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'lsun_bedroom', 'real')):\n",
    "    temp_dict[os.path.join('lsun_bedroom', 'real', img_path).replace(os.sep, '/')] = 0\n",
    "\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'lsun_bedroom', 'vqdiffusion')):\n",
    "    temp_dict[os.path.join('lsun_bedroom', 'vqdiffusion', img_path).replace(os.sep, '/')] = 1\n",
    "\n",
    "\n",
    "DiffusionForensics_dict['lsun_bedroom_vqdiffusion'] = temp_dict\n",
    "\n",
    "\n",
    "temp_dict={}\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'lsun_bedroom', 'real')):\n",
    "    temp_dict[os.path.join('lsun_bedroom', 'real', img_path).replace(os.sep, '/')] = 0\n",
    "\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'lsun_bedroom', 'if')):\n",
    "    temp_dict[os.path.join('lsun_bedroom', 'if', img_path).replace(os.sep, '/')] = 1\n",
    "\n",
    "\n",
    "DiffusionForensics_dict['lsun_bedroom_if'] = temp_dict\n",
    "\n",
    "temp_dict={}\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'lsun_bedroom', 'real')):\n",
    "    temp_dict[os.path.join('lsun_bedroom', 'real', img_path).replace(os.sep, '/')] = 0\n",
    "\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'lsun_bedroom', 'dalle2')):\n",
    "    temp_dict[os.path.join('lsun_bedroom', 'dalle2', img_path).replace(os.sep, '/')] = 1\n",
    "\n",
    "\n",
    "DiffusionForensics_dict['lsun_bedroom_dalle2'] = temp_dict\n",
    "\n",
    "temp_dict={}\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'lsun_bedroom', 'real')):\n",
    "    temp_dict[os.path.join('lsun_bedroom', 'real', img_path).replace(os.sep, '/')] = 0\n",
    "\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'lsun_bedroom', 'midjourney')):\n",
    "    temp_dict[os.path.join('lsun_bedroom', 'midjourney', img_path).replace(os.sep, '/')] = 1\n",
    "\n",
    "\n",
    "DiffusionForensics_dict['lsun_bedroom_midjourney'] = temp_dict\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "\n",
    "temp_dict={}\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'celebahq', 'real')):\n",
    "    temp_dict[os.path.join('celebahq', 'real', img_path).replace(os.sep, '/')] = 0\n",
    "\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'celebahq', 'sdv2')):\n",
    "    temp_dict[os.path.join('celebahq', 'sdv2', img_path).replace(os.sep, '/')] = 1\n",
    "\n",
    "\n",
    "DiffusionForensics_dict['celebahq_sdv2'] = temp_dict\n",
    "\n",
    "temp_dict={}\n",
    "\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'celebahq', 'real')):\n",
    "    temp_dict[os.path.join('celebahq', 'real', img_path).replace(os.sep, '/')] = 0\n",
    "\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'celebahq', 'if')):\n",
    "    temp_dict[os.path.join('celebahq', 'if', img_path).replace(os.sep, '/')] = 1\n",
    "\n",
    "\n",
    "DiffusionForensics_dict['celebahq_if'] = temp_dict\n",
    "\n",
    "temp_dict={}\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'celebahq', 'real')):\n",
    "    temp_dict[os.path.join('celebahq', 'real', img_path).replace(os.sep, '/')] = 0\n",
    "\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'celebahq', 'dalle2')):\n",
    "    temp_dict[os.path.join('celebahq', 'dalle2', img_path).replace(os.sep, '/')] = 1\n",
    "\n",
    "\n",
    "DiffusionForensics_dict['celebahq_dalle2'] = temp_dict\n",
    "\n",
    "temp_dict={}\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'celebahq', 'real')):\n",
    "    temp_dict[os.path.join('celebahq', 'real', img_path).replace(os.sep, '/')] = 0\n",
    "\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'DiffusionForensics', 'celebahq', 'midjourney')):\n",
    "    temp_dict[os.path.join('celebahq', 'midjourney', img_path).replace(os.sep, '/')] = 1\n",
    "\n",
    "\n",
    "DiffusionForensics_dict['celebahq_midjourney'] = temp_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(os.path.join(root, 'DiffusionForensics', 'test.json'), 'w') as json_file:\n",
    "    json.dump(DiffusionForensics_dict, json_file, indent=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67399f2-ac60-4eff-84fa-f135f724a84e",
   "metadata": {},
   "source": [
    "# 4. ForenSynths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4a7be392-9b24-42bb-86e1-eb46784db70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "root='./'\n",
    "ForenSynths_dict = {}\n",
    "for subtask in os.listdir(os.path.join(root, 'ForenSynths', 'test')):\n",
    "    temp_dict={}\n",
    "    if '0_real' in list(os.listdir(os.path.join(root, 'ForenSynths', 'test', subtask))):\n",
    "        for img_path in os.listdir(os.path.join(root, 'ForenSynths', 'test', subtask, '0_real')):\n",
    "            temp_dict[os.path.join('test', subtask, '0_real', img_path).replace(os.sep, '/')] = 0\n",
    "        for img_path in os.listdir(os.path.join(root, 'ForenSynths', 'test', subtask, '1_fake')):\n",
    "            temp_dict[os.path.join('test', subtask, '1_fake', img_path).replace(os.sep, '/')] = 1\n",
    "    else:\n",
    "        for ii in os.listdir(os.path.join(root, 'ForenSynths', 'test', subtask)):\n",
    "            for img_path in os.listdir(os.path.join(root, 'ForenSynths', 'test', subtask, ii, '0_real')):\n",
    "                temp_dict[os.path.join('test', subtask, ii, '0_real', img_path).replace(os.sep, '/')] = 0\n",
    "            for img_path in os.listdir(os.path.join(root, 'ForenSynths', 'test', subtask, ii, '1_fake')):\n",
    "                temp_dict[os.path.join('test', subtask, ii, '1_fake', img_path).replace(os.sep, '/')] = 1\n",
    "    ForenSynths_dict[subtask] = temp_dict\n",
    "\n",
    "\n",
    "with open(os.path.join(root, 'ForenSynths', 'test.json'), 'w') as json_file:\n",
    "    json.dump(ForenSynths_dict, json_file, indent=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bae54ac4-de9f-4551-9f77-d23acaef04d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2065256e-f780-496e-8fab-0f4e43fa9a03",
   "metadata": {},
   "source": [
    "# 5. AntifakePrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b44a9fb7-b13f-4275-9594-49f26f90347d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "3000\n",
      "3000\n",
      "3000\n",
      "3000\n",
      "3000\n",
      "3000\n",
      "3000\n",
      "3000\n",
      "3000\n",
      "3000\n",
      "3000\n",
      "3000\n",
      "3000\n",
      "3000\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "def find_images(root, sub):\n",
    "    image_pattern = re.compile(r'.*\\.(jpg|jpeg|png|bmp)$', re.IGNORECASE)\n",
    "    image_paths = []\n",
    "    for current_root, dirs, files in os.walk(os.path.join(root, sub)):\n",
    "        for file in files:\n",
    "            if image_pattern.match(file):\n",
    "                full_path = os.path.join(current_root, file)\n",
    "                if os.path.isfile(full_path):\n",
    "                    relative_path = os.path.relpath(full_path, root)\n",
    "                    image_paths.append(relative_path.replace(os.sep, '/'))\n",
    "    return image_paths\n",
    "\n",
    "data_path = os.path.join(root, 'AntifakePrompt')\n",
    "\n",
    "AntifakePrompt_dict = {}\n",
    "\n",
    "real = find_images(data_path, 'COCO')\n",
    "print(len(real))\n",
    "temp_dict={}\n",
    "for img_path in real:\n",
    "    temp_dict[img_path] = 0\n",
    "\n",
    "\n",
    "AntifakePrompt_dict['COCO'] = temp_dict\n",
    "\n",
    "real = find_images(data_path, 'flickr30k_224')\n",
    "print(len(real))\n",
    "temp_dict={}\n",
    "for img_path in real:\n",
    "    temp_dict[img_path] = 0\n",
    "\n",
    "\n",
    "AntifakePrompt_dict['Flickr'] = temp_dict\n",
    "\n",
    "fake = find_images(data_path, 'AdvAtk_Imagenet')\n",
    "print(len(fake))\n",
    "temp_dict={}\n",
    "for img_path in fake:\n",
    "    temp_dict[img_path] = 1\n",
    "\n",
    "\n",
    "AntifakePrompt_dict['AdvAtk'] = temp_dict\n",
    "\n",
    "fake = find_images(data_path, os.path.join('DALLE2', 'commonFake_COCO')) + find_images(data_path, os.path.join('DALLE2', 'commonFake_Flickr'))\n",
    "print(len(fake))\n",
    "temp_dict={}\n",
    "for img_path in fake:\n",
    "    temp_dict[img_path] = 1\n",
    "\n",
    "\n",
    "AntifakePrompt_dict['DALLE2'] = temp_dict\n",
    "\n",
    "fake = find_images(data_path, 'deeperforensics_faceOnly')\n",
    "print(len(fake))\n",
    "temp_dict={}\n",
    "for img_path in fake:\n",
    "    temp_dict[img_path] = 1\n",
    "\n",
    "\n",
    "AntifakePrompt_dict['Deeperforensics'] = temp_dict\n",
    "\n",
    "fake = find_images(data_path, os.path.join('IF', 'commonFake_COCO')) + find_images(data_path, os.path.join('IF', 'commonFake_Flickr'))\n",
    "print(len(fake))\n",
    "temp_dict={}\n",
    "for img_path in fake:\n",
    "    temp_dict[img_path] = 1\n",
    "\n",
    "\n",
    "AntifakePrompt_dict['IF'] = temp_dict\n",
    "\n",
    "fake = find_images(data_path, 'lte_SR4_224')\n",
    "print(len(fake))\n",
    "temp_dict={}\n",
    "for img_path in fake:\n",
    "    temp_dict[img_path] = 1\n",
    "\n",
    "\n",
    "AntifakePrompt_dict['lte'] = temp_dict\n",
    "\n",
    "fake = find_images(data_path, 'SD2Inpaint_224')\n",
    "print(len(fake))\n",
    "temp_dict={}\n",
    "for img_path in fake:\n",
    "    temp_dict[img_path] = 1\n",
    "\n",
    "\n",
    "AntifakePrompt_dict['SD2Inpaint'] = temp_dict\n",
    "\n",
    "fake = find_images(data_path, os.path.join('SDXL', 'commonFake_COCO')) + find_images(data_path, os.path.join('SDXL', 'commonFake_Flickr'))\n",
    "print(len(fake))\n",
    "temp_dict={}\n",
    "for img_path in fake:\n",
    "    temp_dict[img_path] = 1\n",
    "\n",
    "\n",
    "AntifakePrompt_dict['SDXL'] = temp_dict\n",
    "\n",
    "fake = find_images(data_path, 'Backdoor_Imagenet')\n",
    "print(len(fake))\n",
    "temp_dict={}\n",
    "for img_path in fake:\n",
    "    temp_dict[img_path] = 1\n",
    "\n",
    "\n",
    "AntifakePrompt_dict['Backdoor'] = temp_dict\n",
    "\n",
    "fake = find_images(data_path, 'Control_COCO')\n",
    "print(len(fake))\n",
    "temp_dict={}\n",
    "for img_path in fake:\n",
    "    temp_dict[img_path] = 1\n",
    "\n",
    "\n",
    "AntifakePrompt_dict['Control'] = temp_dict\n",
    "\n",
    "fake = find_images(data_path, 'DataPoison_Imagenet')\n",
    "print(len(fake))\n",
    "temp_dict={}\n",
    "for img_path in fake:\n",
    "    temp_dict[img_path] = 1\n",
    "\n",
    "\n",
    "AntifakePrompt_dict['DataPoison'] = temp_dict\n",
    "\n",
    "fake = find_images(data_path, 'lama_224')\n",
    "print(len(fake))\n",
    "temp_dict={}\n",
    "for img_path in fake:\n",
    "    temp_dict[img_path] = 1\n",
    "\n",
    "\n",
    "AntifakePrompt_dict['Lama'] = temp_dict\n",
    "\n",
    "fake = find_images(data_path, os.path.join('SD2', 'commonFake_COCO')) + find_images(data_path, os.path.join('SD2', 'commonFake_Flickr'))\n",
    "print(len(fake))\n",
    "temp_dict={}\n",
    "for img_path in fake:\n",
    "    temp_dict[img_path] = 1\n",
    "\n",
    "\n",
    "AntifakePrompt_dict['SD2'] = temp_dict\n",
    "\n",
    "fake = find_images(data_path, 'SD2SuperRes_SR4_224')\n",
    "print(len(fake))\n",
    "temp_dict={}\n",
    "for img_path in fake:\n",
    "    temp_dict[img_path] = 1\n",
    "\n",
    "\n",
    "AntifakePrompt_dict['SD2SuperRes'] = temp_dict\n",
    "\n",
    "fake = find_images(data_path, 'SGXL')\n",
    "print(len(fake))\n",
    "temp_dict={}\n",
    "for img_path in fake:\n",
    "    temp_dict[img_path] = 1\n",
    "\n",
    "\n",
    "AntifakePrompt_dict['SGXL'] = temp_dict\n",
    "\n",
    "\n",
    "with open(os.path.join(root, 'AntifakePrompt', 'test.json'), 'w') as json_file:\n",
    "    json.dump(AntifakePrompt_dict, json_file, indent=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a257f0d7-d8a0-4cad-9e90-7ef2e74dafd5",
   "metadata": {},
   "source": [
    "# 6. CelebDF v1\n",
    "\n",
    "List_of_testing_videos.txt 的label和别人是反着的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5618785e-e609-4567-8ef0-5e004ca8731d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [03:11<00:00,  1.91s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "CelebDFV1_dict = {}\n",
    "temp_dict={}\n",
    "\n",
    "with open(os.path.join(root, 'Celeb-DF-v1', 'List_of_testing_videos.txt'), 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for line in tqdm(lines):\n",
    "    label, id = line.split()\n",
    "    dir_name, file_name = os.path.split(id)\n",
    "    base_name, _ = os.path.splitext(file_name)\n",
    "    for frame_id in os.listdir(os.path.join(root, 'Celeb-DF-v1', dir_name, 'frames')):\n",
    "        for img_path in os.listdir(os.path.join(root, 'Celeb-DF-v1', dir_name, 'frames', frame_id)):\n",
    "            if os.path.isfile(os.path.join(root, 'Celeb-DF-v1', dir_name, 'frames', frame_id, img_path)):\n",
    "                if label == \"1\":\n",
    "                    temp_dict[os.path.join(dir_name, 'frames', frame_id, img_path).replace(os.sep, '/')] = 0\n",
    "                else:\n",
    "                    temp_dict[os.path.join(dir_name, 'frames', frame_id, img_path).replace(os.sep, '/')] = 1\n",
    "\n",
    "CelebDFV1_dict['all'] = temp_dict\n",
    "\n",
    "with open(os.path.join(root, 'Celeb-DF-v1', 'test.json'), 'w') as json_file:\n",
    "    json.dump(CelebDFV1_dict, json_file, indent=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1927caa6-d1bb-4c70-9cca-8bca5648f68e",
   "metadata": {},
   "source": [
    "# 7. CelebDF v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5019c34e-914f-4cec-b8e7-187d1e37bce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [03:09<00:00,  1.90s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "CelebDFV2_dict = {}\n",
    "temp_dict={}\n",
    "\n",
    "with open(os.path.join(root, 'Celeb-DF-v2', 'List_of_testing_videos.txt'), 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for line in tqdm(lines):\n",
    "    label, id = line.split()\n",
    "    dir_name, file_name = os.path.split(id)\n",
    "    base_name, _ = os.path.splitext(file_name)\n",
    "    for frame_id in os.listdir(os.path.join(root, 'Celeb-DF-v2', dir_name, 'frames')):\n",
    "        for img_path in os.listdir(os.path.join(root, 'Celeb-DF-v2', dir_name, 'frames', frame_id)):\n",
    "            if os.path.isfile(os.path.join(root, 'Celeb-DF-v2', dir_name, 'frames', frame_id, img_path)):\n",
    "                if label == \"1\":\n",
    "                    temp_dict[os.path.join(dir_name, 'frames', frame_id, img_path).replace(os.sep, '/')] = 0\n",
    "                else:\n",
    "                    temp_dict[os.path.join(dir_name, 'frames', frame_id, img_path).replace(os.sep, '/')] = 1\n",
    "\n",
    "CelebDFV2_dict['all'] = temp_dict\n",
    "\n",
    "with open(os.path.join(root, 'Celeb-DF-v2', 'test.json'), 'w') as json_file:\n",
    "    json.dump(CelebDFV2_dict, json_file, indent=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb92b6f4-8963-49d8-a321-69a9e27d8726",
   "metadata": {},
   "source": [
    "# 8. FF++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca3dd9-cd0b-4d3c-9f08-05e1d432f090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f05ba3e-5b56-46e9-822c-9e68875e97ee",
   "metadata": {},
   "source": [
    "# 9. AIGCDetect\n",
    "\n",
    "https://github.com/Ekko-zn/AIGCDetectBenchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a29c1027-7935-4f7e-843b-55b4ccfba5cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 系统找不到指定的路径。: './AIGCDetect\\\\test'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m root\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      7\u001B[0m ForenSynths_dict \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m subtask \u001B[38;5;129;01min\u001B[39;00m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlistdir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mroot\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mAIGCDetect\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtest\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m      9\u001B[0m     temp_dict\u001B[38;5;241m=\u001B[39m{}\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m0_real\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(os\u001B[38;5;241m.\u001B[39mlistdir(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(root, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAIGCDetect\u001B[39m\u001B[38;5;124m'\u001B[39m, subtask))):\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [WinError 3] 系统找不到指定的路径。: './AIGCDetect\\\\test'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "root='./'\n",
    "ForenSynths_dict = {}\n",
    "for subtask in os.listdir(os.path.join(root, 'AIGCDetect')):\n",
    "    temp_dict={}\n",
    "    if '0_real' in list(os.listdir(os.path.join(root, 'AIGCDetect', subtask))):\n",
    "        for img_path in os.listdir(os.path.join(root, 'AIGCDetect',  subtask, '0_real')):\n",
    "            temp_dict[os.path.join(subtask, '0_real', img_path).replace(os.sep, '/')] = 0\n",
    "        for img_path in os.listdir(os.path.join(root, 'AIGCDetect', subtask, '1_fake')):\n",
    "            temp_dict[os.path.join(subtask, '1_fake', img_path).replace(os.sep, '/')] = 1\n",
    "    else:\n",
    "        for ii in os.listdir(os.path.join(root, 'AIGCDetect', subtask)):\n",
    "            for img_path in os.listdir(os.path.join(root, 'AIGCDetect', subtask, ii, '0_real')):\n",
    "                temp_dict[os.path.join(subtask, ii, '0_real', img_path).replace(os.sep, '/')] = 0\n",
    "            for img_path in os.listdir(os.path.join(root, 'AIGCDetect', subtask, ii, '1_fake')):\n",
    "                temp_dict[os.path.join(subtask, ii, '1_fake', img_path).replace(os.sep, '/')] = 1\n",
    "    ForenSynths_dict[subtask] = temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db436f9-e215-4ee7-a21c-e87b2ae9d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(root, 'AIGCDetect', 'test.json'), 'w') as json_file:\n",
    "    json.dump(ForenSynths_dict, json_file, indent=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3776f91-edb2-4c6d-86e8-93d7458df526",
   "metadata": {},
   "source": [
    "# 9. DFDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22b744c-22c7-4054-ac7d-3d7055745e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "758a02d8-02d3-4a83-a84d-fa50f8547ddf",
   "metadata": {},
   "source": [
    "# 10. DFDCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb05fe5-f3c5-4e9c-bc56-77b7f0a28481",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "api = HfApi()\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=\"E:\\data\\AntifakePrompt\",\n",
    "    path_in_repo=\"./AntifakePrompt\",\n",
    "    repo_id=\"nebula/DFDatasets\",\n",
    "    repo_type=\"dataset\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b15d1c5d977fe802"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 11. Diffusion1kStep"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3024e548a4acdeb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "root='./'\n",
    "ForenSynths_dict = {}\n",
    "for subtask in os.listdir(os.path.join(root, 'Diffusion1kStep')):\n",
    "    temp_dict={}\n",
    "    if '0_real' in list(os.listdir(os.path.join(root, 'Diffusion1kStep', subtask))):\n",
    "        for img_path in os.listdir(os.path.join(root, 'Diffusion1kStep',  subtask, '0_real')):\n",
    "            temp_dict[os.path.join(subtask, '0_real', img_path).replace(os.sep, '/')] = 0\n",
    "        for img_path in os.listdir(os.path.join(root, 'Diffusion1kStep', subtask, '1_fake')):\n",
    "            temp_dict[os.path.join(subtask, '1_fake', img_path).replace(os.sep, '/')] = 1\n",
    "    else:\n",
    "        for ii in os.listdir(os.path.join(root, 'Diffusion1kStep', subtask)):\n",
    "            for img_path in os.listdir(os.path.join(root, 'Diffusion1kStep', subtask, ii, '0_real')):\n",
    "                temp_dict[os.path.join(subtask, ii, '0_real', img_path).replace(os.sep, '/')] = 0\n",
    "            for img_path in os.listdir(os.path.join(root, 'Diffusion1kStep', subtask, ii, '1_fake')):\n",
    "                temp_dict[os.path.join(subtask, ii, '1_fake', img_path).replace(os.sep, '/')] = 1\n",
    "    ForenSynths_dict[subtask] = temp_dict\n",
    "    \n",
    "\n",
    "\n",
    "with open(os.path.join(root, 'Diffusion1kStep', 'test.json'), 'w') as json_file:\n",
    "    json.dump(ForenSynths_dict, json_file, indent=4) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce159d96e84b2dfb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "794071b9149b4378"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 12. GANGen-Detection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f4875573b8b4596"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "root='./'\n",
    "ForenSynths_dict = {}\n",
    "for subtask in os.listdir(os.path.join(root, 'GANGen-Detection')):\n",
    "    temp_dict={}\n",
    "    if '0_real' in list(os.listdir(os.path.join(root, 'GANGen-Detection', subtask))):\n",
    "        for img_path in os.listdir(os.path.join(root, 'GANGen-Detection',  subtask, '0_real')):\n",
    "            temp_dict[os.path.join(subtask, '0_real', img_path).replace(os.sep, '/')] = 0\n",
    "        for img_path in os.listdir(os.path.join(root, 'GANGen-Detection', subtask, '1_fake')):\n",
    "            temp_dict[os.path.join(subtask, '1_fake', img_path).replace(os.sep, '/')] = 1\n",
    "    else:\n",
    "        for ii in os.listdir(os.path.join(root, 'GANGen-Detection', subtask)):\n",
    "            for img_path in os.listdir(os.path.join(root, 'GANGen-Detection', subtask, ii, '0_real')):\n",
    "                temp_dict[os.path.join(subtask, ii, '0_real', img_path).replace(os.sep, '/')] = 0\n",
    "            for img_path in os.listdir(os.path.join(root, 'GANGen-Detection', subtask, ii, '1_fake')):\n",
    "                temp_dict[os.path.join(subtask, ii, '1_fake', img_path).replace(os.sep, '/')] = 1\n",
    "    ForenSynths_dict[subtask] = temp_dict\n",
    "    \n",
    "\n",
    "\n",
    "with open(os.path.join(root, 'GANGen-Detection', 'test.json'), 'w') as json_file:\n",
    "    json.dump(ForenSynths_dict, json_file, indent=4) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ee51a156977d9f2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b22774f6c096a135"
  },
  {
   "cell_type": "markdown",
   "source": [
    "AI_recognition_dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23bde2f73471006d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "root='./'\n",
    "data_dict = {}\n",
    "\n",
    "temp_dict={}\n",
    "for img_path in os.listdir(os.path.join(root, 'AI_recognition_dataset', 'real')):\n",
    "    temp_dict[os.path.join('real', img_path).replace(os.sep, '/')] = 0\n",
    "\n",
    "for img_path in os.listdir(os.path.join(root, 'AI_recognition_dataset', 'fake')):\n",
    "    temp_dict[os.path.join('fake', img_path).replace(os.sep, '/')] = 1\n",
    "\n",
    "data_dict['all'] = temp_dict\n",
    "\n",
    "\n",
    "with open(os.path.join(root, 'AI_recognition_dataset', 'test.json'), 'w') as json_file:\n",
    "    json.dump(data_dict, json_file, indent=4) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "214fa25f2e637f58"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dbf3a6067bca657c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Artifact"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcab7dd2c7a31716"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "\n",
    "real_names = [\"afhq\", \"coco\", \"celebahq\", \"ffhq\", \"imagenet\", \"landscape\", \"lsun\", \"metfaces\"]\n",
    "\n",
    "root='./'\n",
    "data_dict = {}\n",
    "\n",
    "for subset in real_names:\n",
    "    temp_dict={}\n",
    "    with open(os.path.join(root, subset, 'metadata.csv'), newline='', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        _ = next(reader)\n",
    "        for row in reader:\n",
    "            if int(row[2]) == 0:\n",
    "                temp_dict[os.path.join(subset, row[1])] = 0\n",
    "            else:\n",
    "                print(row)\n",
    "    data_dict[subset] = temp_dict\n",
    "\n",
    "\n",
    "temp_dict={}\n",
    "with open(os.path.join(root, 'cycle_gan', 'metadata.csv'), newline='', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    _ = next(reader)\n",
    "    for row in reader:\n",
    "        if int(row[2]) == 0:\n",
    "            temp_dict[os.path.join('cycle_gan', row[1])] = 0\n",
    "        else:\n",
    "            print(row)\n",
    "\n",
    "data_dict['real_cyclegan'] = temp_dict\n",
    "\n",
    "\n",
    "fake_names = [\n",
    "    \"diffusion_gan\", \"gau_gan\", \"lama\", \"mat\", \"projected_gan\", \"stylegan1\", \n",
    "    \"vq_diffusion\", \"big_gan\", \"face_synthetics\", \"generative_inpainting\", \n",
    "    \"sfhq\", \"stylegan2\", \"ddpm\", \"glide\", \"latent_diffusion\", \"palette\", \n",
    "    \"stable_diffusion\", \"stylegan3\", \"cips\", \"denoising_diffusion_gan\", \n",
    "    \"gansformer\", \"pro_gan\", \"star_gan\", \"taming_transformer\"\n",
    "]\n",
    "\n",
    "for subset in fake_names:\n",
    "    temp_dict={}\n",
    "    with open(os.path.join(root, subset, 'metadata.csv'), newline='', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        _ = next(reader)\n",
    "        for row in reader:\n",
    "            if int(row[2]) in [1,2,3,4,5,6]:\n",
    "                temp_dict[os.path.join(subset, row[1])] = 1\n",
    "            elif int(row[2]) == 0:\n",
    "                temp_dict[os.path.join(subset, row[1])] = 0\n",
    "            else:\n",
    "                print(row)\n",
    "    data_dict[subset] = temp_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "temp_dict={}\n",
    "with open(os.path.join(root, 'cycle_gan', 'metadata.csv'), newline='', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    _ = next(reader)\n",
    "    for row in reader:\n",
    "        if int(row[2]) == 6:\n",
    "            temp_dict[os.path.join('cycle_gan', row[1])] = 1\n",
    "        else:\n",
    "            print(row)\n",
    "\n",
    "data_dict['cyclegan'] = temp_dict\n",
    "\n",
    "\n",
    "with open(os.path.join(root, 'test.json'), 'w') as json_file:\n",
    "    json.dump(data_dict, json_file, indent=4) \n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8465197388771ccf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6bb1a51180f963b3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "DIF_testset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8f205f458072443"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "root='./'\n",
    "data_dict = {}\n",
    "for subtask in os.listdir(os.path.join(root, 'DIF_testset')):\n",
    "    temp_dict={}\n",
    "    for img_path in os.listdir(os.path.join(root, 'DIF_testset',  subtask, '0_real')):\n",
    "        temp_dict[os.path.join(subtask, '0_real', img_path).replace(os.sep, '/')] = 0\n",
    "    for img_path in os.listdir(os.path.join(root, 'DIF_testset', subtask, '1_fake')):\n",
    "        temp_dict[os.path.join(subtask, '1_fake', img_path).replace(os.sep, '/')] = 1\n",
    "    data_dict[subtask] = temp_dict\n",
    "\n",
    "\n",
    "\n",
    "with open(os.path.join(root, 'DIF_testset', 'test.json'), 'w') as json_file:\n",
    "    json.dump(data_dict, json_file, indent=4) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdd9fd0ab78b02b3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fake2M"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e018171ffcf95f2f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def find_images(directory_path):\n",
    "    base_path = pathlib.Path(directory_path).resolve()  \n",
    "    path = pathlib.Path(directory_path)\n",
    "    path = path.resolve()\n",
    "    image_extensions = {\".jpg\", \".jpeg\", \".png\", \".webp\"}\n",
    "    images = [str(file.relative_to(base_path)) for file in path.rglob('*') if file.suffix.lower() in image_extensions]\n",
    "    return images\n",
    "\n",
    "\n",
    "\n",
    "root='./'\n",
    "data_dict = {}\n",
    "for subtask in os.listdir(os.path.join(root, 'Fake2M')):\n",
    "    temp_dict={}\n",
    "    images = find_images(os.path.join(root, 'Fake2M', subtask))\n",
    "    for img_path in images:\n",
    "        temp_dict[os.path.join(subtask, img_path).replace(os.sep, '/')] = 1\n",
    "    data_dict[subtask] = temp_dict\n",
    "\n",
    "\n",
    "\n",
    "with open(os.path.join(root, 'Fake2M', 'test.json'), 'w') as json_file:\n",
    "    json.dump(data_dict, json_file, indent=4) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f517e0c679b2d915"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "726fda59294bc688"
  },
  {
   "cell_type": "markdown",
   "source": [
    "synthbuster"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99e792e140d8afc5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "names = ['dalle2','firefly','stable-diffusion-1-4','stable-diffusion-xl', 'dalle3','glide','midjourney-v5','stable-diffusion-1-3','stable-diffusion-2', 'real']\n",
    "\n",
    "root='./'\n",
    "data_dict = {}\n",
    "\n",
    "\n",
    "for subtask in names:\n",
    "    temp_dict={}\n",
    "    try:\n",
    "        for img_path in os.listdir(os.path.join(root, 'synthbuster',  subtask, '0_real')):\n",
    "            temp_dict[os.path.join(subtask, '0_real', img_path).replace(os.sep, '/')] = 0\n",
    "    except:\n",
    "        print('failed')\n",
    "    try:\n",
    "        for img_path in os.listdir(os.path.join(root, 'synthbuster', subtask, '1_fake')):\n",
    "            temp_dict[os.path.join(subtask, '1_fake', img_path).replace(os.sep, '/')] = 1\n",
    "    except:\n",
    "        print('failed')\n",
    "    data_dict[subtask] = temp_dict\n",
    "\n",
    "\n",
    "\n",
    "with open(os.path.join(root, 'synthbuster', 'test.json'), 'w') as json_file:\n",
    "    json.dump(data_dict, json_file, indent=4) \n",
    "\n",
    "\n",
    "\n",
    "# \n",
    "# \n",
    "# subset = 'dalle2'\n",
    "# for fake_image in os.listdir(os.path.join(root, 'synthbuster', subset, '1_fake')):\n",
    "#     real_image = os.path.basename(fake_image).split('.')[0]+'.jpg'\n",
    "#     os.makedirs(os.path.join(root, 'synthbuster', subset, '0_real'), exist_ok=True)\n",
    "#     try:\n",
    "#         shutil.copy(os.path.join('/home/jwang/ybwork/data/deepfake_benchmark/real', real_image),\n",
    "#                     os.path.join(root, 'synthbuster', subset, '0_real', real_image))\n",
    "#     except:\n",
    "#         print('failed')\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f749a7863bbd040"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "50fc7cd18457d8ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "AIArt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd134734b91b0c5c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "root='./'\n",
    "data_dict = {}\n",
    "for subtask in os.listdir(os.path.join(root, 'AIArt')):\n",
    "    temp_dict={}\n",
    "    for img_path in os.listdir(os.path.join(root, 'AIArt',  subtask, '0_real')):\n",
    "        temp_dict[os.path.join(subtask, '0_real', img_path).replace(os.sep, '/')] = 0\n",
    "    for img_path in os.listdir(os.path.join(root, 'AIArt', subtask, '1_fake')):\n",
    "        temp_dict[os.path.join(subtask, '1_fake', img_path).replace(os.sep, '/')] = 1\n",
    "    data_dict[subtask] = temp_dict\n",
    "\n",
    "\n",
    "\n",
    "with open(os.path.join(root, 'AIArt', 'test.json'), 'w') as json_file:\n",
    "    json.dump(data_dict, json_file, indent=4) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3dfd6caf2422804"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e5b34cc65e01de4d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f3382c5f678c1479"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
