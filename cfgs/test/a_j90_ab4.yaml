arch: "arp"

#resume: './logs/arpaug-ViTL-a01b1c1-881_20240514_08_32_18/epoch=00-val_acc_epoch=1.00.ckpt'
#resume: './logs/arpaug-ViTL-a05b05c1-881_20240514_02_58_41/epoch=00-val_acc_epoch=1.00.ckpt'
#resume: './logs/arpaug-ViTL-a05b1c1-881_20240514_07_23_24/epoch=00-val_acc_epoch=1.00.ckpt'
resume: './logs/arpaug-ViTL-a0b1c1-881_20240514_02_47_45/epoch=00-val_acc_epoch=1.00.ckpt'
#resume: './logs/arpaug-ViTL-a1b05c1-881_20240514_06_15_04/epoch=00-val_acc_epoch=1.00.ckpt'
#resume: './logs/arpaug-ViTL-a1b0c1-881_20240514_02_30_25/epoch=00-val_acc_epoch=1.00.ckpt'
#resume: './logs/arpaug-ViTL-a1b1c1-881-first12cls_20240514_03_28_50/epoch=00-val_acc_epoch=1.00.ckpt'
#resume: './logs/arpaug-ViTL-a1b1c1-881-first16cls_20240514_04_11_21/epoch=00-val_acc_epoch=1.00.ckpt'
#resume: './logs/arpaug-ViTL-a1b1c1-881-first2cls_20240511_23_01_23/epoch=00-val_acc_epoch=1.00.ckpt'
#resume: './logs/arpaug-ViTL-a1b1c1-881-first4cls_20240511_22_43_55/epoch=00-val_acc_epoch=1.00.ckpt'
#resume: './logs/arpaug-ViTL-a1b1c1-881-first8cls_20240511_23_12_42/epoch=00-val_acc_epoch=1.00.ckpt'
#resume: './logs/arpaug-ViTL-a2b2c1-881_20240514_05_06_45/epoch=00-val_acc_epoch=1.00.ckpt'

test_name: "arpaug_ab_a0b1c1_881"

specific_cls: False

model:
  NAME: "ViT-L/14"
  N_CTX_VISION: 16
  N_CTX_TEXT: 16
  CTX_INIT: False
  PROMPT_DEPTH_VISION: 8
  PROMPT_DEPTH_TEXT: 8
  PROMPT_NUM_TEXT: 1

dataset:
  train:
    multicalss_names: ["airplane", "bird", "bottle", "car", "chair", "diningtable", "horse", "person", "sheep", "train",
                       "bicycle", "boat", "bus", "cat", "cow", "dog", "motorbike", "pottedplant", "sofa", "tvmonitor"]

datasets:
  batch_size: 32
  loader_workers: 8
  data_root: "/home/jwang/ybwork/data/DFBenchmark"
#  data_root: "/scratch/yw26g23/datasets/deepfakebenchmark"

  trsf:
    - _target_: data.Compress
      method: "JPEG"
      qf: 90
    - _target_: torchvision.transforms.Resize
      size: 256
    - _target_: torchvision.transforms.CenterCrop
      size: 224
    - _target_: torchvision.transforms.ToTensor
    - _target_: torchvision.transforms.Normalize
      mean: [0.48145466, 0.4578275, 0.40821073]
      std: [0.26862954, 0.26130258, 0.27577711]


  source:
    ForenSynths:
      data_root: '${datasets.data_root}/ForenSynths'
      sub_sets: [ "biggan", "crn", "cyclegan", "deepfake", "gaugan", "imle", "progan", "san", "seeingdark", "stargan",
                  "stylegan", "stylegan2", "whichfaceisreal" ]

    DIF:
      data_root: '${datasets.data_root}/DIF_testset'
      sub_sets: ['biggan',  'cyclegan',  'dalle_2',  'dalle_mini',  'gaugan',  'glide',  'mj',  'progan',  'sd14',  'sd21',
                 'stargan',  'stylegan',  'stylegan2']

    GenImage:
      data_root: '${datasets.data_root}/AIGCDetect'
      sub_sets: [ 'wukong', 'biggan', 'Midjourney', 'stable_diffusion_v_1_5', 'ADM',  'Glide','stable_diffusion_v_1_4', 'VQDM' ]

    Ojha:
      data_root: '${datasets.data_root}/Ojha'
      sub_sets: ["dalle", "glide_100_10", "glide_100_27", "glide_50_27", "guided", "ldm_100", "ldm_200", "ldm_200_cfg"]

