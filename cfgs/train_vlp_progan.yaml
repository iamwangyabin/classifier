arch: "vlp"  # Architecture for binary classification, e.g., 'resnet50.tv_in1k' 'vit_base_patch16_224'
resume: null
name: "vlp-ViTLâ€”Progan"  # Name of the experiment for organizing samples and models

train:
  gpu_ids: [0]  # GPU IDs to use for training, e.g., '0' for single GPU or '0,1,2,3' for multiple GPUs
  train_epochs: 5  # Number of training epochs
  gradient_accumulation_steps: 1
  check_val_every_n_epoch: 1

  optimizer:
    _target_: torch.optim.AdamW
    _partial_: True
    lr: 1e-4
    weight_decay: 1e-3

  scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingLR
    _partial_: True
    T_max: 5

model:
  NAME: "ViT-L/14"
  N_CTX_VISION: 2
  N_CTX_TEXT: 2
  CTX_INIT: "a photo of a"
  PROMPT_DEPTH_VISION: 2
  PROMPT_DEPTH_TEXT: 2


dataset:
  train:
    mode: 'binary'
    dataroot: "/scratch/yw26g23/datasets/deepfakebenchmark/"
    subfolder_names: ["ForenSynths"]
    multicalss_idx: [1]
#    dataroot: "/home/jwang/ybwork/data/CDDB"
#    subfolder_names: ["imle"]
#    multicalss_idx: [0]
    batch_size: 256
    loader_workers: 32
    loadSize: 256
    cropSize: 224
    random_flip: true
    augment: null

  val:
    mode: 'binary'
    dataroot: "/scratch/yw26g23/datasets/deepfakebenchmark/ForenSynths/test"
    subfolder_names: ["biggan", "deepfake", "whichfaceisreal", "gaugan"]
    multicalss_idx: [0, 0, 0, 0]
    batch_size: 256
    loader_workers: 4
    loadSize: 256
    cropSize: 224


